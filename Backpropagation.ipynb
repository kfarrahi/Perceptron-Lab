{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in more detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of an MLP as extending the Perceptron by making the following changes:\n",
    "\n",
    "- adding an additional layer to the network which can have an arbitrary number of neurons. For simplicity you can assume the hidden layer has 2 neurons + 1 bias.\n",
    "- modifying the activation function of the neurons from the step function to the sigmoid function. Note, a function defining the sigmoid is given below. \n",
    "- incorporating the backpropagation algorithm for learning as follows\n",
    "\n",
    "The forward pass of the algorithm simply computes what the output of the MLP will generate following the equation we covered in the lecture. Let us assume the following notation:\n",
    "We have 2 inputs $x_i$ and a bias $x_o$. We have 2 neurons in the hidden layer, $h_1$ and $h_2$, with weights between the input layer and hidden layer denoted as $w_{ji}^{(1)}$, where $j$ represents the node in the hidden layer that the weight is connected to and $i$ represents the node in the input layer that the weight initiates from. Let the weights between the hidden layer and output layer be denoted by $w_{kj}^{(2)}$, where $k$ represents the node in the output layer that the weight is connected to and $j$ represents the node in the hidden layer that the weight initiates from. Note, in this example $k=1$. We can now compute some equations for the forward pass, which computes the output of the MLP.<br>\n",
    "\n",
    "\n",
    "FORWARD PASS <br>\n",
    "$y_{h_j} = f(\\sum_{i=0}^d w^{(1)}_{ji}⋅x_i)$ <br>\n",
    "$y_{k = 1} = f(\\sum_{j=0}^{n_H} w_{j}^{(2)} ⋅ f(\\sum_{i=0}^d w_{ji}^{(1)} x_i)))$\n",
    "\n",
    "Note, here the index $j$ corresponds to the index of the neuron in the hidden layer and not the data point. For this lab you can assume that $f$ in both cases is the sigmoid function, defined by the function named sigmoid in the code below.\n",
    "\n",
    "BACKWARD PASS <br>\n",
    "Then, to update the weights, we need to compute the loss. Let's assume the following expression, where $y$ for each of the data points $m$ follows the expression above for $y_{k=1}$:\n",
    "\n",
    "$L = \\frac{1}{2} \\sum_{m=1}^{(no. data points)} (y_m - t_m)^2$\n",
    "\n",
    "Note, the $\\frac{1}{2}$ term just allows the expression to be simplified later on as we need to compute the derivative of the loss to update the weights.\n",
    "\n",
    "$w \\leftarrow w - \\eta * \\Delta w$ <br><br>\n",
    "\n",
    "For the weights between the hidden layer and the output layer, the update equation is: <br>\n",
    "$\\Delta w_{kj}^{(2)} = \\delta_o⋅y_{h_j}$ <br>\n",
    "\n",
    "$\\delta_o = (y−t)⋅(y)⋅(1-y)$ <br>\n",
    "\n",
    "$y_{h_j}$ is the output of the hidden layer neuron $j$ defined above. <br><br>\n",
    "\n",
    "For the weights between the input layer and the hidden layer, the update equation is: <br>\n",
    "$\\Delta w_{ji}^{(1)} = (\\delta_o ⋅ w_{k=1,j}^{(2)}) ⋅(y_{h_j})⋅(1-y_{h_j})⋅x_i$ <br>\n",
    "\n",
    "\n",
    "You can work with the snippets of code given below. However, if you prefer to rewrite the code yourself that is great.\n",
    "\n",
    "For more details about the derivation see this page: https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "\n",
    "Note, another good source for the backpropagation algorithm can be found here: http://neuralnetworksanddeeplearning.com/chap1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
