{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP3223/COMP6245 Machine Learning: Lab Sheet - The Perceptron"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Kate Farrahi, 23 Nov 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is inspired by the following blog post: \n",
    "\n",
    "(1) https://medium.com/@thomascountz/19-line-line-by-line-python-perceptron-b6f113b161f3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Foundations of Machine Learning module, you learnt about the perceptron, the neuron, and a little bit about the multilayer perceptron (MLP) so far. This lab will remind you of some of those ideas. In the lab, you will implement the perceptron algorithm. You will then test your implementation on various logical expressions to see the limits of the algorithm. \n",
    "\n",
    "Through this lab you'll learn how to:\n",
    "\n",
    "- create and manipulate a perceptron to solve various logical expression\n",
    "- understand the limits of a perceptron\n",
    "\n",
    "To work through this lab you'll use the Python 3 language in a Jupyter Notebook environment, with the numpy package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementing a Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this lab is to implement the perceptron learning algorithm \n",
    "to classify data for the logical 'OR function'. \n",
    "\n",
    "In the first lecture, we learnt about the perceptron. We discussed the formula that describes the process to perform a binary classification of inputs. We learnt that the perceptron takes in an input vector, $x$, multiplies it by a corresponding weight vector $w$, and then adds it to a bias, $b$. It then uses an activation function, (the step function, for the case of the perceptron), to determine if our resulting summation is greater than $0$, in order to classify the output as $1$, or less than or equal to $0$, in order to classify the output as $0$. Note, you can assume that $b = w_o$ and $x_0 = 1$. This results in the following expression:\n",
    "\n",
    "$y_j = f(\\sum_{i=o}^m w_i x_{ij}) = 1$ if $\\sum_{i=o}^m w_i · x_{ij} > 0$ <br>\n",
    "$y_j = 0$ otherwise\n",
    "\n",
    "Next, we learnt that by using labeled data, we could have our perceptron predict an output, determine if it was correct or not, and then adjust the weights and bias accordingly. The update equation is given as follows: \n",
    "\n",
    "$w_i \\leftarrow w_i - \\eta * (y_j - t_j) * x_{ij}$\n",
    "\n",
    "Where $\\eta$ represents the learning rate. Start by filling in the missing code for the perceptron class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "\n",
    "    def __init__(self, no_of_inputs, threshold=100, learning_rate=0.01):\n",
    "        self.threshold = threshold\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.ones(no_of_inputs + 1)\n",
    "           \n",
    "    def predict(self, inputs):\n",
    "        # write the code to implement the activation output (i.e. y_j from the expression above) \n",
    "        ## code goes here\n",
    "        return activation\n",
    "\n",
    "    def train(self, training_inputs, labels):\n",
    "        for _ in range(self.threshold):\n",
    "            for inputs, label in zip(training_inputs, labels):\n",
    "                prediction = self.predict(inputs)\n",
    "                # write the code to compute the weight updates\n",
    "                ## code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code can be used to test your Perceptron\n",
    "import numpy as np\n",
    "\n",
    "training_inputs = []\n",
    "training_inputs.append(np.array([0, 0]))\n",
    "training_inputs.append(np.array([1, 0]))\n",
    "training_inputs.append(np.array([0, 1]))\n",
    "training_inputs.append(np.array([1, 1]))\n",
    "\n",
    "labels = np.array([0, 1, 1, 0])\n",
    "\n",
    "perceptron = Perceptron(2)\n",
    "perceptron.train(training_inputs, labels)\n",
    "\n",
    "inputs = np.array([1, 1])\n",
    "perceptron.predict(inputs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your perceptron is working, try varying the learning rate. \n",
    "\n",
    "After how many iterations does your perceptron converge?\n",
    "\n",
    "Does your perceptron correctly classify the OR function?\n",
    "\n",
    "Does your perceptron correctly classify the XOR function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (EXTRA): Implementing a Multilayer Perceptron with a Single Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of an MLP as extending the Perceptron by making the following changes:\n",
    "\n",
    "- adding an additional layer to the network which can have an arbitrary number of neurons. For simplicity you can assume the hidden layer has 2 neurons + 1 bias.\n",
    "- modifying the activation function of the neurons from the step function to the sigmoid function. Note, a function defining the sigmoid is given below. \n",
    "- incorporating the backpropagation algorithm for learning as follows\n",
    "\n",
    "The forward pass of the algorithm simply computes what the output of the MLP will generate following the equation we covered in the lecture:\n",
    "\n",
    "FORWARD PASS <br>\n",
    "$y = f(\\sum_{j=1}^{n_H} w_{j} ⋅ f(\\sum_{i=1}^d w_{ji} x_i + w_{j0}) + w_{0}))$\n",
    "\n",
    "Note, here the biases are written as $w_{j0}$ and $w_{0}$ for consistency with the lecture slides (where k=1, we only have one output neuron for simplicity). For this lab you can assume that $f$ in both cases is the sigmoid function, defined by the function named sigmoid in the code below.\n",
    "\n",
    "BACKWARD PASS <br>\n",
    "Then, to update the weights, we need to compute the loss. Let's assume the following expression:\n",
    "\n",
    "$L = \\frac{1}{2} \\sum_{j} (y_j - t_j)^2$\n",
    "\n",
    "Note, the $\\frac{1}{2}$ term just allows the expression to be simplified later on as we need to compute the derivative of the loss to update the weights.\n",
    "\n",
    "$w \\leftarrow w - \\eta * \\Delta w$ \n",
    "\n",
    "where for the update of the weights between the hidden layer and the output layer differs from the update of the weights between the hidden layer and input layer. You can differentiate the total loss $L$ with respect to the weight $w$ you are updating to find $\\Delta w$.\n",
    "\n",
    "You can work with the snippets of code given below. However, if you prefer to rewrite the code yourself that is great.\n",
    "\n",
    "For more details about the derivation see this page: https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "\n",
    "Note, another good source for the backpropagation algorithm can be found here: http://neuralnetworksanddeeplearning.com/chap1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    print(x)\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "class MultiLayerPerceptron(object):\n",
    "    def __init__(self,dim_input, dim_output):\n",
    "        self.weights1   = np.random.rand(dim_input,3)\n",
    "        self.weights2   = np.random.rand(3,1)                 \n",
    "        self.output     = np.zeros(dim_output)\n",
    "\n",
    "    def feedforward(self, inputs):\n",
    "        ## implement code here \n",
    "\n",
    "    def backprop(self, inputs, label):\n",
    "        ## implement code here\n",
    "\n",
    "\n",
    "    def train(self,iterations, x, y):\n",
    "        for _ in range(iterations):\n",
    "            ## implement code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
